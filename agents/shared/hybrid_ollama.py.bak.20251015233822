"""
Hybrid Ollama Provider with sns-core Compression
Container-agnostic implementation for both Pydantic AI and Parlant agents

Usage:
    from hybrid_ollama import HybridOllamaProvider, OperationType

    # Background agent (use local Ollama, $0 cost)
    provider = HybridOllamaProvider(operation_type=OperationType.BACKGROUND)
    model = provider.get_model()

    # User-facing agent (use Ollama Cloud, <1s latency)
    provider = HybridOllamaProvider(operation_type=OperationType.USER_FACING)
    model = provider.get_model()

    # Both automatically use sns-core compression
"""

import os
from enum import Enum
from typing import Optional, Dict, Any
from sns_core import CICDNotation


class OperationType(Enum):
    """Operation type determines which Ollama backend to use"""
    BACKGROUND = "background"      # CI/CD agents, async tasks → Local Ollama ($0, 5-10s)
    USER_FACING = "user_facing"    # Conversational, interactive → Ollama Cloud (<1s, ~$0.0002)


class HybridOllamaProvider:
    """
    Hybrid Ollama Provider with automatic backend selection and sns-core compression

    Architecture:
    - Background operations → Local Ollama (localhost:11434)
    - User-facing operations → Ollama Cloud (api.ollama.com)
    - All operations → sns-core compression (60-85% token reduction)

    Environment Variables:
    - OLLAMA_LOCAL_URL: Local Ollama endpoint (default: http://localhost:11434)
    - OLLAMA_CLOUD_URL: Ollama Cloud endpoint (default: https://api.ollama.com)
    - OLLAMA_CLOUD_API_KEY: Ollama Cloud API key (optional, free tier available)
    - OLLAMA_MODEL: Model to use (default: qwen2.5:7b-instruct)
    - RUNNING_IN_DOCKER: Set to 'true' if running in Docker (uses host.docker.internal)
    """

    def __init__(
        self,
        operation_type: OperationType = OperationType.BACKGROUND,
        model: Optional[str] = None,
        enable_sns_core: bool = True,
        custom_config: Optional[Dict[str, Any]] = None
    ):
        """
        Initialize Hybrid Ollama Provider

        Args:
            operation_type: BACKGROUND (local, free) or USER_FACING (cloud, fast)
            model: Model name (default: qwen2.5:7b-instruct)
            enable_sns_core: Enable sns-core compression (default: True)
            custom_config: Custom configuration overrides
        """
        self.operation_type = operation_type
        self.model_name = model or os.getenv('OLLAMA_MODEL', 'qwen2.5:7b-instruct')
        self.enable_sns_core = enable_sns_core
        self.custom_config = custom_config or {}

        # Initialize sns-core
        self.sns = CICDNotation() if enable_sns_core else None

        # Determine backend configuration
        self._configure_backend()

    def _configure_backend(self):
        """Configure backend based on operation type"""

        # Check if running in Docker
        is_docker = os.getenv('RUNNING_IN_DOCKER', 'false').lower() == 'true'

        if self.operation_type == OperationType.BACKGROUND:
            # Use local Ollama for background operations
            base_url = os.getenv('OLLAMA_LOCAL_URL', 'http://localhost:11434')

            # Docker environment: replace localhost with host.docker.internal
            if is_docker:
                base_url = base_url.replace('localhost', 'host.docker.internal')
                base_url = base_url.replace('127.0.0.1', 'host.docker.internal')

            self.base_url = base_url
            self.api_key = None
            self.timeout = self.custom_config.get('timeout', 60)
            self.latency_target = '5-10s'
            self.cost = '$0'

        else:  # USER_FACING
            # Use Ollama Cloud for user-facing operations
            base_url = os.getenv('OLLAMA_CLOUD_URL', 'https://api.ollama.com')
            api_key = os.getenv('OLLAMA_CLOUD_API_KEY')

            self.base_url = base_url
            self.api_key = api_key
            self.timeout = self.custom_config.get('timeout', 10)
            self.latency_target = '<1s'
            self.cost = '~$0.0002/query'

    def get_model_string(self) -> str:
        """
        Get model string for Pydantic AI

        Returns:
            Model string in format: "ollama:model_name"
        """
        return f"ollama:{self.model_name}"

    def get_model(self) -> str:
        """
        Get Pydantic AI model string and configure environment

        Returns:
            Model string for Pydantic AI using OpenAI-compatible format

        Note: Ollama provides an OpenAI-compatible API, so we use the OpenAI
        provider in Pydantic AI and configure it to point to the Ollama endpoint.
        """
        # Configure OpenAI-compatible environment variables for Ollama
        # Pydantic AI will use these to connect to Ollama's OpenAI-compatible API
        os.environ['OPENAI_BASE_URL'] = f"{self.base_url}/v1"
        os.environ['OPENAI_API_KEY'] = "ollama"  # Ollama doesn't require a real key

        # Return model in OpenAI format
        return f"openai:{self.model_name}"

    def get_config(self) -> Dict[str, Any]:
        """
        Get configuration dictionary

        Returns:
            Configuration with base_url, headers, timeout, etc.
        """
        config = {
            'base_url': f"{self.base_url}/api",
            'timeout': self.timeout,
            'model': self.model_name,
            'operation_type': self.operation_type.value,
            'sns_core_enabled': self.enable_sns_core,
        }

        # Add authorization header for Ollama Cloud
        if self.api_key and 'ollama.com' in self.base_url:
            config['headers'] = {
                'Authorization': f'Bearer {self.api_key}'
            }

        return config

    def compress_prompt(self, prompt: str, context: Optional[Dict[str, Any]] = None) -> str:
        """
        Compress prompt using sns-core notation

        Args:
            prompt: Original prompt text
            context: Optional context dictionary to compress

        Returns:
            Compressed prompt string
        """
        if not self.enable_sns_core or not self.sns:
            return prompt

        # If context provided, encode it
        if context:
            encoded_context = self.sns.encode_dict(context)
            return f"{prompt}|ctx:{encoded_context}"

        return prompt

    def get_stats(self) -> Dict[str, Any]:
        """
        Get provider statistics

        Returns:
            Statistics including backend, cost, latency targets
        """
        return {
            'backend': 'Local Ollama' if self.operation_type == OperationType.BACKGROUND else 'Ollama Cloud',
            'endpoint': self.base_url,
            'model': self.model_name,
            'operation_type': self.operation_type.value,
            'latency_target': self.latency_target,
            'cost': self.cost,
            'sns_core_enabled': self.enable_sns_core,
            'compression_rate': '60-85%' if self.enable_sns_core else '0%',
            'timeout': self.timeout,
        }

    def __repr__(self) -> str:
        stats = self.get_stats()
        return f"HybridOllamaProvider(backend={stats['backend']}, model={stats['model']}, sns_core={stats['sns_core_enabled']})"


class HybridOllamaFactory:
    """
    Factory for creating Hybrid Ollama providers based on agent type

    Automatically determines operation type based on agent purpose
    """

    # Map agent types to operation types
    AGENT_TYPE_MAP = {
        # Background CI/CD agents → Local Ollama
        'build': OperationType.BACKGROUND,
        'test': OperationType.BACKGROUND,
        'deploy': OperationType.BACKGROUND,
        'monitor': OperationType.BACKGROUND,

        # User-facing agents → Ollama Cloud
        'parlant': OperationType.USER_FACING,
        'chat': OperationType.USER_FACING,
        'conversational': OperationType.USER_FACING,
        'interactive': OperationType.USER_FACING,
    }

    @classmethod
    def create_for_agent(
        cls,
        agent_type: str,
        model: Optional[str] = None,
        enable_sns_core: bool = True
    ) -> HybridOllamaProvider:
        """
        Create provider for specific agent type

        Args:
            agent_type: Agent type (build, test, deploy, monitor, parlant, chat, etc.)
            model: Optional model override
            enable_sns_core: Enable sns-core compression

        Returns:
            Configured HybridOllamaProvider
        """
        operation_type = cls.AGENT_TYPE_MAP.get(
            agent_type.lower(),
            OperationType.BACKGROUND  # Default to background
        )

        return HybridOllamaProvider(
            operation_type=operation_type,
            model=model,
            enable_sns_core=enable_sns_core
        )


# Convenience functions for common use cases
def get_background_ollama(model: Optional[str] = None) -> HybridOllamaProvider:
    """Get Ollama provider for background operations (local, free)"""
    return HybridOllamaProvider(
        operation_type=OperationType.BACKGROUND,
        model=model
    )


def get_user_facing_ollama(model: Optional[str] = None) -> HybridOllamaProvider:
    """Get Ollama provider for user-facing operations (cloud, fast)"""
    return HybridOllamaProvider(
        operation_type=OperationType.USER_FACING,
        model=model
    )


# Example usage and testing
if __name__ == '__main__':
    """Test hybrid Ollama provider with different operation types"""

    print("=" * 80)
    print("HYBRID OLLAMA PROVIDER TEST")
    print("=" * 80)
    print()

    # Test background provider (CI/CD agents)
    print("1. Background Agent Configuration (CI/CD)")
    print("-" * 80)
    bg_provider = get_background_ollama()
    print(bg_provider)
    print("\nConfiguration:")
    for key, value in bg_provider.get_config().items():
        print(f"  {key}: {value}")
    print("\nStatistics:")
    for key, value in bg_provider.get_stats().items():
        print(f"  {key}: {value}")
    print()

    # Test user-facing provider (Parlant)
    print("2. User-Facing Agent Configuration (Parlant)")
    print("-" * 80)
    uf_provider = get_user_facing_ollama()
    print(uf_provider)
    print("\nConfiguration:")
    for key, value in uf_provider.get_config().items():
        print(f"  {key}: {value}")
    print("\nStatistics:")
    for key, value in uf_provider.get_stats().items():
        print(f"  {key}: {value}")
    print()

    # Test sns-core compression
    print("3. sns-core Compression Demo")
    print("-" * 80)
    provider = get_background_ollama()

    original_prompt = "Analyze this commit and determine build strategy"
    context = {
        'repo': 'billionmail',
        'commit': 'abc123',
        'files_changed': ['api.py', 'models.py'],
        'lines_changed': '+45,-12'
    }

    compressed = provider.compress_prompt(original_prompt, context)

    print(f"Original: {original_prompt} + context")
    print(f"Context size: {len(str(context))} chars")
    print(f"\nCompressed: {compressed}")
    print(f"Compressed size: {len(compressed)} chars")
    print(f"Reduction: {((len(str(context)) - len(compressed)) / len(str(context)) * 100):.1f}%")
    print()

    # Test factory
    print("4. Factory Pattern Test")
    print("-" * 80)

    agent_types = ['build', 'test', 'parlant', 'chat']
    for agent_type in agent_types:
        provider = HybridOllamaFactory.create_for_agent(agent_type)
        stats = provider.get_stats()
        print(f"{agent_type.upper():12} → {stats['backend']:15} | {stats['cost']:12} | {stats['latency_target']}")
    print()

    print("=" * 80)
    print("TEST COMPLETE")
    print("=" * 80)
